\documentclass[main.tex]{subfiles}

\begin{document}
\minispacing

\section{Markov Chains and Random Walks}

A \tb{stochastic process} $\mathbf{X}=\{X(t) : t \in T\}$ is a collection of random variables $X(t)$ (interchangeably, $X_t$), the \tb{state} of process at time $t$. Assume stochastic processes below are discrete time and discrete space.

A discrete time stochastic process $X_0,X_1,X_2,\cdots$ is a (time-homogeneous) \tb{Markov chain} if and only if $\Pb(X_t=a_t\mid X_{t-1}=a_{t-1},X_{t-2}=a_{t-2},\cdots,X_0=a_0)=\Pb(X_t=a_t\mid X_{t-1}=a_{t-1})=P_{a_{t-1},a_{t}}$. The state $X_t$ only depends on the previous state $X_{t-1}$. This is called the \tb{Markov property} or \tb{memoryless property}, and we say that chain is \tb{Markovian}. The transition probabilities form a one-step \tb{transition matrix} $P$, and for all $i$, $\sum_{j\ge 0}P_{i,j}=1$. Let $p(t) = (p_0(t),p_1(t),p_2(t),\cdots)$ represents the distribution of the state at time $t$, and we have $p(t)=p(t-1)\cdot P$.

In the finite case, it is equivalent to analyzing the connectivity structure of the directed graph (i.e. strongly connected component). It follows several trivial definitions and conclusions. State $j$ is \tb{accessible} from state $i$ if $\exists\;\! n\ge 0,P^n_{i,j}>0$. If two states $i$ and $j$ are accessible from each other, we say that they \tb{communicate}. A Markov chain is \tb{irreducible} if all states belong to one communicating class. Let $r^t_{i,j}=\Pb(X_t=j \wedge \forall 1\le s\le t-1,X_s\not=j \mid X_0=i)$. A state is \tb{recurrent} if $\sum_{t\ge 1}r^t_{i,i}=1$, and \tb{transient} otherwise. Let $h_{i,j}=\sum_{t\ge 1}t\cdot r^t_{i,j}$. A recurrent state $i$ is \tb{positive recurrent} if $h_{i,i}<\infty$. Otherwise, it is \tb{null recurrent} (this occurs only in infinite case).

\begin{lemma}
	In a finite Markov chain, at least one state is recurrent, and all recurrent states are positive recurrent.
\end{lemma}

A state $j$ in a discrete time Markov chain is \tb{periodic} if there exists and integer $\Delta > 1$ such that $\Pb(X_{t+s}=j\mid X_t=j)=0$ unless $\Delta \mid s$. A discrete time Markov chain is periodic if any state in the chain is periodic. A state of chain that is not periodic is \tb{aperiodic}. An aperiodic, positive recurrent state is an \tb{ergodic} state. A Markov chain is ergodic if all its states are ergodic.

A \tb{stationary distribution} $\pi$ of a Markov chain is a probability distribution $\pi$ such that $\pi=\pi\cdot P$.

\begin{theorem}
	Any finite, irreducible, and ergodic Markov chain has the following properties:
	\begin{itemize}
		\item the chain has a unique stationary distribution $\pi=(\pi_0,\pi_1,\cdots,\pi_n)$;
		\item for all $j$ and $i$, the limit $\lim_{t\ra\infty}P^t_{j,i}$ exists and it is independent of $j$;
		\item $\pi_i=\lim_{t\ra\infty}P^t_{j,i}=1/h_{i,i}$.
	\end{itemize}
\end{theorem}

\begin{lemma}
	For any irreducible, ergodic Markov chain and for any state $i$, the limit $\lim_{t\ra\infty}P^t_{i,i}=1/h_{i,i}$.
\end{lemma}

The expected time between visits to $i$ is $h_{i,i}$ and therefore state $i$ is visited $1/h_{i,i}$ of the time. Thus, if $\lim_{t\ra\infty}P^t_{i,i}$ exists, it must be $1/h_{i,i}$. In fact, any finite Markov chain has a stationary distribution; but in the case of periodic state $i$, the stationary probability $\pi_i$ is not the limiting probability of being in $i$ (which does not exist) but instead just the long-term frequency of visiting state $i$. We can compute the stationary distribution by solving $\pi\cdot P = \pi$.

Considering the \tb{cut-sets} of Markov chain, for any state $i$ of the chain, $\sum_{j=0}^{n}\pi_jP_{j,i}=\pi_i=\pi_i\sum_{j=0}^{n}P_{i,j}$. It follows

\begin{theorem}
	Let $S$ be a set of states of a finite, irreducible, aperiodic Markov chain. In the stationary distribution, the probability that the chain leaves the set $S$ equals the probability that it enters $S$.
\end{theorem}

\begin{theorem}
	Consider a finite, irreducible, and ergodic Markov chain with transition matrix $P$. If there are nonnegative numbers $\pi=(\pi_0,\cdots,\pi_n)$ such that $\sum_{i=0}^{n}\pi_i=1$ and if, for any pair of states $i,j$, $\pi_iP_{i,j}=\pi_jP_{j,i}$, then $\pi$ is the stationary distribution corresponding to $P$.
\end{theorem}

Chains that satisfy the condition $\pi_iP_{i,j}=\pi_jP_{j,i}$ are called \tb{time reversible}.

\begin{theorem}
	Any irreducible aperiodic Markov chain belongs to one of the following two categories:
	\begin{itemize}
		\item the chain is ergodic -- for any pair of states $i$ and $j$, the limit $\lim_{t\ra\infty}P^t_{j,i}$ exists and is independent of $j$, and the chain has a unique stationary distribution $\pi_i=\lim_{t\ra\infty}P^t_{j,i}>0$; or 
		\item no state is positive recurrent -- for all $i$ and $j$, $\lim_{t\ra\infty}P^t_{j,i}=0$, and the chain has no stationary distribution.
	\end{itemize}
\end{theorem}

A \tb{random walk} on $G$ is a Markov chain, where $P_{i,j}=1\,/\,\mathrm{deg}(i)$.

\begin{lemma}
	A random walk on an undirected graph $G$ is aperiodic if and only if $G$ is not bipartite.
\end{lemma}

\begin{theorem}
	A random walk on $G$ converges to a stationary distribution $\pi$, where $\pi_v=\mathrm{deg}(v)\,/\,2 |E|$.
\end{theorem}

Denote \tb{hitting time} $h_{u,v}$ the expected time to reach state $v$ when starting at state $u$. The \tb{cover time} of a graph $G$ is the maximum over nodes $v\in V$ of the expected time to visit all of the nodes by a random walk starting from $v$.

\begin{lemma}
	If $(u,v)\in E$, the commute time $h_{u,v}+h_{v,u}$ is at most $2 |E|$.
\end{lemma}

\begin{pf}
	We can view the random walk on $G$ as a Markov chain with states of $2 |E|$ directed edges. Since it is a \tb{doubly} stochastic (the sum of the entries in each column is $1$), it has a uniform stationary distribution. An upper bound for $h_{u,v}+h_{v,u}$ is the interval of visiting time of edge $(u,v)$.
\end{pf}

\begin{lemma}
	The cover time of $G=(V,E)$ is bounded above by $2|E|(|V|-1)$.
\end{lemma}

\begin{theorem}\na{Matthews' theorem}
	The cover time $C_G$ of $G=(V,E)$ with $n$ vertices is bounded by
	\[C_G\le H(n-1)\max_{u,v\in V: u\not=v}h_{u,v}.\]
\end{theorem}

\begin{pf}
	Consider a random permutation $\{Z_1,Z_2,\cdots,Z_n\}$. Assume that we have computed the expected time visiting all of $\{Z_1,\cdots, Z_{j-1}\}$. If $Z_j$ is not the first visiting node in $\{Z_1,\cdots, Z_j\}$, it contributes nothing. Otherwise, it contributes to the answer with the probability of $1/j$.
\end{pf}

\tb{Parrondo's paradox} shows that two losing games can be combined to make a winning game.

\bigskip

{\bs A random algorithm for 3-Satisfiability} ...

\bigskip

\ex{7.13} (a) $\Pb(X_k\mid X_{k+1},\cdots,X_m) = \Pb(X_k,\cdots,X_m) \,/\, \Pb(X_{k+1},\cdots,X_m) = \Pb(X_k) \Pb(X_{k+1} \mid X_{k}) \Pb(X_{k+2},\cdots,$ $X_{m} \mid X_{k}, X_{k+1}) \,/\, \Pb(X_{k+1}) \Pb(X_{k+2},\cdots,X_{m} \mid X_{k+1}) = \Pb(X_k) \Pb(X_{k+1} \mid X_k) \,/\, \Pb(X_{k+1})$, thus it is Markovian. (b) Let $\Pb(X_k=j)=\pi_j$ and $\Pb(X_{k+1}=j)=\pi_j$. (c) From part (b), we have $\pi_i Q_{i,j} = \pi_j P_{j,i}$. Then $Q_{i,j} = P_{i,j}$.

\ex{7.17} Recall that we let $r^t_{0,0}$ be the probability that the first return to 0 from 0 is at time t. Then
\[\sum_{t=0}^{\infty} r^t_{0,0} = \sum_{n=0}^{\infty}C_np^n(1-p)^{n+1}=(1-p)\cdot \frac{1-\sqrt{1-4p(1-p)}}{2p(1-p)},\qquad \text{since } \sum_{n=0}^{\infty} C_n x^n = \frac{1-\sqrt{1-4x}}{2x}.\]
Hence the chain is recurrent if and only if $p\le 1/2$. Let $h^t_{0,0}$ be the expectation. Then
\[\sum_{t=0}^{\infty} h^t_{0,0} = \sum_{n=0}^{\infty}(2n+2)C_np^n(1-p)^{n+1}=\frac{2(1-p)}{\sqrt{1-4p(1-p)}},\qquad \text{since }(n+1)C_n=\binom{2n}{n} \text{ and } \sum_{n=0}^{\infty}\binom{2n}{n} = \frac{1}{\sqrt{1-4x}}.\]
Hence $h^t_{0,0}$ is finite when $p < 1/2$ and is infinite when $p = 1/2$.

\ex{7.18} \na{Random walk on $\Z^d$} Let $P_d(n)$ be the probability that one returns to origin at time $n$. Random walk on $\Z^d$ is recurrent if and only if $\sum_{n\ge 1}P_d(2n)$ is unbound. In case of $d=2$, we can transform Manhattan distance into Chebyshev distance (i.e. $(x,y)\ra(x+y,x-y)$), thus it becomes two independent random walks on $\Z$. Then
\[
	\sum_{n=1}^{\infty}P_2(2n)=\sum_{n=1}^{\infty}P_1(2n)^2=\sum_{n=1}^{\infty}\biggl(\frac{\binom{2n}{n}}{2^{2n}}\biggr)^2\simeq\sum_{n=1}^{\infty}\frac{1}{\pi n} = \infty,\qquad \text{since }n!\sim n^ne^{-n}\sqrt{2\pi n} \text{ (Stirlingâ€™s formula)}
\]
In the case of $d=3$, we have $P_3(n)=\Theta(n^{-3/2})$. The explicit expectation formula can be derived by Fourier analysis.

\ex{7.22} Formulate a new Markov chain with $n^2$ states of the form $(i, j)$. By Lemma 1.9, $h_{u,v}\le 4m^2$, and we can construct a length $O(n)$ path from $(i,j)$ to $(i,i)$, which gives us an upper bound of $O(m^2n)$.

\ex{7.24} \na{Lollipop graph} (a) We need to travel from $v$ to $u$ first, and then travel around the clique. Thus $C_G=h_{v,u}+c_u=\Theta(n^2)+\Theta(n\log n) = \Theta(n^2)$. (b) $h_{u,v}\le C_G \le h_{u,v}+c_u$, thus $C_G=\Theta(h_{u,v}) = \Theta(n^3)$.

\ex{7.30} \na{Random walk on hypercube} Let $f_i$ be the hitting time when there is exactly $i$ bits differ. Then,
\[
	f_i=\frac{i}{n}\cdot f_{i-1} + \frac{n-i}{n}\cdot f_{i+1} + 1
	\Ra i\cdot(f_i-f_{i-1})=(n-i)\cdot(f_{i+1}-f_{i})+n.
\]
Denote the difference $f_i-f_{i-1}$ by $g_i$. Then we have $g_n=1$, and $i\cdot g_i = (n-i) \cdot g_{i+1} + n$. Expand formula, it follows
\[
	g_1=\binom{n-1}{1}\cdot g_2+\binom{n}{1}=\binom{n-1}{2}\cdot g_3+\binom{n}{2}+\binom{n}{1}=\cdots=\sum_{i=1}^{n}\binom{n}{i}=2^{n}-1.
\]
In addition, $g_n\le g_{n-1}\le \cdots \le g_2 \le \frac{g_1}{n-1}$. Thus, $f_n=\sum_{i=1}^{n} g_i = \Theta(2^n)$ and the cover time is $O(N \log N)$.

\end{document}