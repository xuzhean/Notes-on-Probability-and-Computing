\documentclass[main.tex]{subfiles}

\begin{document}
\minispacing

\section{Moments and Deviations}

\begin{theorem}\tb{(Markov's Inequality)}
	Let $X$ be a random variable with only nonnegative values. Then, for all $a > 0$,
	\[
		\Pb(X \ge a) \le \frac{\E[X]}{a}
	\]
\end{theorem}

\begin{pf}
	For $a > 0$, let $I=1$ (if $X\ge a$) or $0$ (otherwise), and note that $I \le X / a$. Taking expectaions on both sides, thus yields $\Pb(X \ge a) = \E[I] = \le \E[X / a] = \E[X] / a$.
\end{pf}

The \tb{$k$-th moment} of a random variable $X$ is $\E[X^k]$. The \tb{variance} of random variable $X$ is defined as $\Var[X]=\E[(X-\E[X])^2]=\E[X^2]-\E[X]^2$, and the \tb{standard deviation} of a random variable $X$ is $\sigma[X]=\sqrt{\Var[X]}$. The \tb{convariance} of two random variables $X$ and $Y$ is $\Cov(X,y)=\E[(X-\E[X])(Y-\E[Y])]$, and we have

\begin{lemma}
	For any two random variables $X$ and $Y$, $\Var[X+Y]=\Var[X]+\Var[Y]+2\cdot\Cov(X,Y)$.
\end{lemma}

\begin{lemma}
	For any two independent random variables $X$ and $Y$, $\E[X \cdot Y] = \E[X] \cdot \E[Y]$. (opposite does not hold)
\end{lemma}

\begin{corollary}
	If $X$ and $Y$ are independent random variables, then $\Cov(X,Y)=0$.
\end{corollary}

\begin{theorem}\na{Linearity of variance}
	Let $X_1,X_2,\cdots,X_n$ be mutually independent random variables. Then
	\[
		\Var\biggl[\;\!\sum_{i=1}^{n}X_i\biggr]=\sum_{i=1}^{n}\Var[X_i]
	\]
\end{theorem}

For example, a Bernoulli trial with success probability $p$ has variable $p(1-p)$, therefore the variance of a binomial random variable $X$ with parameters $n$ and $p$ is $np(1-p)$.

\begin{theorem}\na{Chebyshev's inequality}
	Let $X$ be a random variable. Then, for any $a > 0$,
	\[
		\Pb\left(\lvert X-\E[X]\rvert\ge a\right)\le \frac{\Var[X]}{a^2}
	\]
\end{theorem}

\begin{pf}
	We can apply Markov's inequality to prove:
	\[
		\Pb(|X-\E[X]|\ge a)=\Pb((X-\E[X])^2\ge a^2)\le\frac{\E[(X-\E[X])^2]}{a^2}=\frac{\Var[X]}{a^2}
	\]
	A useful variant of Chebyshev’s inequality is to substitute $a$ with $t\cdot\sigma[X]\;(t\ge 1)$.
\end{pf}

The \tb{median} of random variable $X$ is defined to be any value $m$ such that $\Pb(X\le m)\ge 1/2$ and $\Pb(X\ge m)\ge 1/2$.

\begin{theorem}
	For any random variable $X$ with finite expectaion $\E[X]$ and finite median $m$,
	\begin{itemize}
		\item the expectaion $\E[X]$ is the value of $c$ that minimizes the expression $\E[(X-c)^2]$.
		\item the median $m$ is the value of $c$ that minimizes the expression $\E[\lvert X-c \rvert]$.
	\end{itemize}
\end{theorem}

\begin{corollary}
	$\lvert \mu-m\rvert = \lvert \E[X]-m\rvert = \lvert\E[X-m]\rvert \le \E[\lvert X-m\rvert]\le \E[\lvert X-\mu\rvert]\le \sqrt{\E[(X-\mu)^2]}=\sigma$.
\end{corollary}

\bigskip

\ex{3.10} By the memoryless property, we have $\E[X^k]=(1-p)\cdot \E[(X+1)^k]+p$. A clever way is to use falling factorial, and we will get $\E[X^{\underline k}]=k!\cdot (1-p)^{k-1}\cdot p^{-k},\; \E[X^n]=\sum_{k=0}^{n}{n \brace k}\cdot \E[X^{\underline k}]$.

\ex{3.15} $\Var[\sum_i X_i]=\sum_i\Var[X_i]+2\sum_i\sum_j\Cov(X_i,X_j)$. If $\E[X_iX_j]=\E[X_i]\E[X_j]$, then $\Cov(X_i,X_j)=0$.

\ex{3.18} \na{Cantelli's inequality} Let $Y = X - \E[X]$, and it follows that $\E[Y] = 0$ and $\Var[Y]=\E[Y^2]=\sigma^2$. For any $\lambda,u>0$ (taking $u=\sigma^2/\lambda$ in last step),
\[
	\Pb(Y\ge\lambda)=\Pb(Y+u\ge\lambda+u)\le\Pb\left((Y+u)^2\ge(\lambda+u)^2\right)\le\frac{\E[(Y+u)^2]}{(\lambda+u)^2}=\frac{\sigma^2+u^2}{(\lambda+u)^2}=\frac{\sigma^2}{\lambda^2+\sigma^2}
\]

\ex{3.26} \na{The weak law of large numbers} Apply Chebyshev’s Inequality, thus for any $\eps>0$ we have
\[
	\Pb\left(\left\lvert\frac{X_1+X_2+\cdots+X_n}{n}\right\rvert-\mu\right)\le\frac{\sigma^2}{\eps^2\cdot n}\ra 0,\qquad \textrm{as } n\ra\infty.
\]

\end{document}

