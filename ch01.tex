\documentclass[main.tex]{subfiles}

\begin{document}
\minispacing

\section{Events and Probability}


A \href{https://en.wikipedia.org/wiki/Probability_space}{\it probability space} is a \ti{measure space} $(\Om,\F,\Pb)$ consisting of:

\begin{itemize}
	\item the \ti{sample space} $\Om$ --- a set of outcomes called \ti{sample};
	\item the \href{https://en.wikipedia.org/wiki/%CE%A3-algebra}{$\sigma$-\ti{algebra}} $\F$ --- a family of subsets of $\Om$, called \ti{events}, such that $\Om \in \F$ and $\F$ is closed under complements (i.e. $\forall A \in \F,\, \Om \backslash A \in \F$) and countable unions (i.e. $\forall  A_i \in \F,\, \bigcup_{i=1}^{\infty} A_i \in \F$);
	\item the \ti{probability function} $\Pb: \F \ra [0,1]$ such that $\Pb(\Om)=1$ and $\Pb$ is \href{https://en.wikipedia.org/wiki/Sigma-additive_set_function}{$\sigma$-\ti{additive}} (i.e. $\Pb(\bigsqcup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} \Pb(A_i)$).
\end{itemize}

The motivation behind this complicated definition is that some sets are \href{https://en.wikipedia.org/wiki/Non-measurable_set}{\it non-measurable}, thus mathematicians developed the theory of \href{https://en.wikipedia.org/wiki/Measure_(mathematics)}{\it measure}. For instance, \href{https://en.wikipedia.org/wiki/Borel_set}{\it Borel set} on real line forms a $\sigma$-algebra which is \href{https://en.wikipedia.org/wiki/Carathéodory%27s_extension_theorem}{\it generated by} open intervals. \href{https://en.wikipedia.org/wiki/Lebesgue–Stieltjes_integration}{\it Stieltjes measures} is a \ti{Borel measure} and builds the measure foundation of \ti{continuous probability distribution}.

\begin{lemma}[Inclusion-exclusion principle]
	Let $E_1,\cdots,E_n$ be any n events. Then
	\[
		\Pb\left(\bigcup_{i=1}^{n}E_i\right) = \sum_{\ell=1}^{n} (-1)^{\ell + 1} \sum_{i_1 < i_2 < \cdots < i_\ell} \Pb \left(\bigcap_{r=1}^{\ell}E_{i_r}\right).
	\]
\end{lemma}

Events $E_1,E_2,\cdots,E_n$ are \ti{mutually independent} (simply called \ti{independent} when $k=2$) if and only if, for any subset $I \subseteq \{1,2,\cdots,k\}$,
$\Pb\left(\bigcap_{i\in I}E_i\right)=\prod_{i\in I}\Pb(E_i)$. Note that events $X,Y,Z,\cdots$ are unnecessarily mutually independent when they are pairwise independent.

The \ti{conditional probability} that event $E$ occurs given that event $F$ occurs is $\Pb(E \mid F) = \Pb(E \cap F) \,/\, \Pb(F)\;(\Pb(F) > 0)$.

\begin{theorem}[Law of total probability]
	Let events $\bigsqcup_{i=1}^{n} E_i = \Om$. Then we have $\Pb(B) = \sum_{i=1}^{n} \Pb(B \mid E_i)\cdot \Pb(E_i)$. 
\end{theorem}

\begin{theorem}[Bayes's law]
	Let events $E_1,E_2,\cdots,E_n$ satisfy $\bigsqcup_{i=1}^{n} E_i = \Om$. Then we have
	\[
		\Pb(E_k \mid B) = \frac{\Pb(E_k \cap B)}{\Pb(B)}
		= \frac{\Pb(B \mid E_k)\cdot \Pb(E_k)}{\sum_{i=1}^{n}\Pb(B \mid E_i) \cdot \Pb(E_i)}.
	\] 
\end{theorem}

In the \href{https://en.wikipedia.org/wiki/Bayesian_inference}{\it Bayesian approach} one starts with a \ti{prior} model, giving some initial value to the model parameters. This model is then modified, by incorporating new observations, to obtain a \ti{posterior} model that captures the new information.

\bigskip

\tb{Exercise 1.6}
Using mathematical induction, we have $p_{i,j} = \frac{i-1}{i+j-1}\cdot p_{i-1,j} + \frac{j-1}{i+j-1}\cdot p_{i,j-1} = \frac{i+j-2}{i+j-1}\cdot\frac{1}{i+j-2} = \frac{1}{i+j-1}$.

\tb{Exercise 1.7.b\ } 
Let $F_{b_1b_2\cdots b_n}$ be the intersection of events $E_i\,(b_i=1)$ or $\Om\backslash E_i\,(b_i=0)$, and $P_{k}$ be the sum of $\Pb(F_b)$ where $b$ consists of $k$ one and $n-k$ zero. Then for every $k \ge 1$, we have $\sum_{i=1}^{l}(-1)^{i+1}\binom{k}{i} = 1 + (-1)^{l+1}\binom{k-1}{l} \ge 1$. Multiply both sides by $P_k$ and sum them up. We eventually reach the desired inequality.

\tb{Exercise 1.11.b\ }
$p_3 = p_1\cdot (1-p_2) + (1-p_1) \cdot p_2 \Ra q_3 = 1-2p_3 = (1-2p_1)(1-2p_2) = q_1q_2$. Is there any underlying motivation?

\tb{Exercise 1.24} \href{https://en.wikipedia.org/wiki/Karger's_algorithm}{(Karger's algorithm)} Let $K$ be the minimum $r$-way cut-set. Considering all $r$-way cut-sets consisting of $r-1$ single vertex, the total size is $m\cdot \binom{n-2}{r-1}$ with an upper bound $(m-|K|)\cdot \binom{n}{r-1}$. It follows that
	\[
	m \cdot \binom{n-2}{r-1} \le (m-|K|)\cdot \binom{n}{r-1} \quad\Ra\quad 1-\frac {|K|}{m} \ge \binom{n-2}{r-1}\binom n{r-1}^{-1}=\frac{(n-r+1)(n-r)}{n(n-1)}.
	\]
The probability that $K$ survives all the $n-r$ iterations is at least
	\[
		\prod_{i=0}^{n-r-1}\frac{(n-i+1-r)(n-i-r)}{(n-i)(n-i-1)} = r\cdot \binom{n}{r-1}^{-1}\binom{n-1}{r-1}^{-1}
	\]
and its reciprocal is the maximum possible number of minimum cardinality of r-way cut-sets.

\end{document}